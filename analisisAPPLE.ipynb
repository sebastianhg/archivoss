{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee5358f3-441a-45b6-a714-2343abadbeaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos guardados exitosamente como 'GOOGL_daily.csv'\n"
     ]
    }
   ],
   "source": [
    "from alpha_vantage.timeseries import TimeSeries\n",
    "import pandas as pd\n",
    "\n",
    "def save_dataset(symbol):\n",
    "    api_key = 'YYYC6JRUF7KDUUZR'  # Recuerda reemplazar esto con tu propia clave de API\n",
    "    ts = TimeSeries(key=api_key, output_format='pandas')\n",
    "    \n",
    "    try:\n",
    "        data, meta_data = ts.get_daily(symbol, outputsize='full')\n",
    "        data.to_csv('./{}_daily.csv'.format(symbol))\n",
    "        print(\"Datos guardados exitosamente como '{}_daily.csv'\".format(symbol))\n",
    "    except Exception as e:\n",
    "        print(\"Error al obtener los datos:\", e)\n",
    "\n",
    "symbol = 'GOOGL'  # El símbolo de Google en Alpha Vantage es 'GOOGL'\n",
    "save_dataset(symbol)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9beb326-78cb-46ca-9a02-c65afd2b15ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d726e82-71a1-4ab1-a2a9-62f05f152b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "nombre_archivo = '{}_daily.csv'.format(simbolo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7107aea8-6acc-4639-a7e6-1edb7cbfcd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "history_points = 50\n",
    "\n",
    "def csv_to_dataset(csv_path):\n",
    "    datos = pd.read_csv(csv_path)\n",
    "\n",
    "    print('Datos: ', datos.head(),'\\n')\n",
    "\n",
    "    # Elimina el primer día\n",
    "    datos  = datos.drop(0, axis=0)\n",
    "\n",
    "    # Ordena de más viejo a  más reciente\n",
    "    datos = datos.sort_values('date')\n",
    "    print('Datos por Fecha: ', datos.head(),'\\n')\n",
    "\n",
    "    # Elimina la columna 'date'  \n",
    "    datos = datos.drop('date', axis=1)\n",
    "\n",
    "    print(\"Tipo de de los Datos:\", type(datos),'\\n')\n",
    "    print(\"Datos: \", datos[:10],'\\n')\n",
    "\n",
    "\n",
    "    normalizador = preprocessing.MinMaxScaler()\n",
    "    datos_norm = normalizador.fit_transform(datos)\n",
    "\n",
    "    print(\"Datos normalizados\", datos_norm[:10],'\\n')\n",
    "    # Usamos los últimos 50 ejemplos para predecir el siguiente valor (open)\n",
    "    # X\n",
    "    ohlcv_histories_normalised =      np.array([datos_norm[i  : i + history_points].copy() for i in range(len(datos_norm) - history_points)])\n",
    "    \n",
    "    \n",
    "    #y\n",
    "    next_day_open_values_normalised = np.array([datos_norm[:,0][i + history_points].copy() for i in range(len(datos_norm) - history_points)])\n",
    "    next_day_open_values = np.array([datos.iloc[:,0][i + history_points].copy() for i in range(len(datos) - history_points)])\n",
    "    next_day_open_values = np.expand_dims(next_day_open_values_normalised, -1)\n",
    "    print('next_day_open_values shape: ',next_day_open_values.shape)\n",
    "    print(next_day_open_values[:10], sep = '\\n')\n",
    "\n",
    "    \n",
    "    y_scaler = preprocessing.MinMaxScaler()\n",
    "    y_scaler.fit( next_day_open_values  )\n",
    "\n",
    "    # Indicadores tecnicos\n",
    "    technical_indicators = []\n",
    "    for his in ohlcv_histories_normalised:\n",
    "        # Promedio del Precio de cierre \n",
    "        sma = np.mean(his[:,3])\n",
    "        technical_indicators.append(np.array([sma]))\n",
    "\n",
    "    technical_indicators = np.array(technical_indicators)\n",
    "\t\t\n",
    "    tech_ind_scaler = preprocessing.MinMaxScaler()\n",
    "    technical_indicators_normalised = tech_ind_scaler.fit_transform(technical_indicators)\n",
    "\n",
    "\n",
    "\n",
    "    # Verifica que el número de xs sea igual al número de ys\n",
    "    assert ohlcv_histories_normalised.shape[0] == next_day_open_values_normalised.shape[0] == technical_indicators_normalised.shape[0]\n",
    "\n",
    "    return ohlcv_histories_normalised, technical_indicators_normalised, next_day_open_values_normalised, next_day_open_values, y_scaler  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6cccbff-74a1-478f-b903-2271284e5987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mapa_contornos.png', 'objetos_azules', 'untitled.md', 'Busqueda_de_planos_a_partir_de_una_nube_de_puntos.ipynb', 'analisisAPPLE.ipynb', 'bynary.ipynb', 'Untitled.ipynb', 'generaExtracionContornos.ipynb', 'contornosTIme6.ipynb', 'conjunto_2.nc', 'generarImagen.ipynb', 'analisisnetcdf.ipynb', 'Untitled6.ipynb', 'procesObjet.py', 'componentConnected.ipynb', 'conjunto_6.nc', 'imagenBIn.py', 'saturation.ipynb', 'nombre_del_archivo.nc', 'objetos_contornos', 'graficarDATOS.ipynb', 'time6', 'analisisTecnico.ipynb', 'conjunto_4.nc', 'conjunto_3.nc', 'objetos_binarizados', 'genContour.ipynb', 'analisiss.rar', 'GENERA_OBJET_BINARIZADO.ipynb', 'genImg1.ipynb', 'prueba.ipynb', 'genNETCDF.ipynb', 'csv2.ipynb', 'Untitled5.ipynb', 'GENERA_COMPONENTES_CONECTADOS.ipynb', 'objetoSinContorno.ipynb', 'generarBinarizado.ipynb', 'conjunto_5.nc', '.ipynb_checkpoints', 'time6.ipynb', 'imagen_coloreada_contornos_cafe.png', 'predicionBolsa.ipynb', 'objetos_componentes_conectados', 'imagen_objetos_azules_contorneados.png', 'ivt_s_6_2022_Jan.nc', 'GENERA_CONTORNO.ipynb', 'prepBinarizado.ipynb', 'graficarBDatos.ipynb', 'Untitled4.ipynb', 'objetos_azules.png', 'genContorno.ipynb', 'objetos_axis', 'generarComponentesconectados.ipynb', 'mercadoFinanciero.ipynb', 'extraerTexto.ipynb', 'Untitled3.ipynb', 'guardarBinarizado.ipynb', 'intensidadObjetos.ipynb', 'mezclarImag.ipynb', 'csv1.ipynb', 'componentsConected.py', 'pdades.ipynb', 'coffe.ipynb', 'errorDetect.ipynb', 'test.ipynb', 'contornos.ipynb', 'extraerMpaBlanco.ipynb', 'testt.ipynb', 'Untitled2.ipynb', 'AAPL_daily.csv', 'contornos_fondo_negro', 'genCDFobjet.ipynb', 'DatosGraficadosConMapa', 'Untitled1.ipynb', 'Untitled7.ipynb', 'GENERA_CONTORNO_FONDO_NEGRO.ipynb', 'GENERA_OBJET_AZULES.ipynb', 'conjunto_1.nc', 'axisJuntos.ipynb', '3D.ipynb', 'sustituciones.ipynb', 'GOOGL_daily.csv', 'GENERA_AXIS.ipynb', 'unirCSV.ipynb', 'aumentarGraficos.ipynb']\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'IXIC_daily.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir())\n\u001b[0;32m----> 4\u001b[0m ohlcv_histories, technical_indicators, next_day_open_values, unscaled_y, y_scaler \u001b[38;5;241m=\u001b[39m \u001b[43mcsv_to_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m_daily.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43msimbolo\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m test_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.9\u001b[39m\n\u001b[1;32m      7\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(ohlcv_histories\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m test_split)\n",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m, in \u001b[0;36mcsv_to_dataset\u001b[0;34m(csv_path)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcsv_to_dataset\u001b[39m(csv_path):\n\u001b[0;32m----> 8\u001b[0m     datos \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDatos: \u001b[39m\u001b[38;5;124m'\u001b[39m, datos\u001b[38;5;241m.\u001b[39mhead(),\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Elimina el primer día\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'IXIC_daily.csv'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir())\n",
    "\n",
    "ohlcv_histories, technical_indicators, next_day_open_values, unscaled_y, y_scaler = csv_to_dataset('{}_daily.csv'.format(simbolo))\n",
    "\n",
    "test_split = 0.9\n",
    "n = int(ohlcv_histories.shape[0] * test_split)\n",
    "\n",
    "ohlcv_train = ohlcv_histories[:n]\n",
    "tech_ind_train = technical_indicators[:n]\n",
    "y_train = next_day_open_values[:n]\n",
    "\n",
    "ohlcv_test = ohlcv_histories[n:]\n",
    "tech_ind_test = technical_indicators[n:]\n",
    "y_test = next_day_open_values[n:]\n",
    "\n",
    "unscaled_y_test = unscaled_y[n:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df93176f-7cd6-459d-8deb-7ea1af258f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorflow_version` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 2.16.1.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b51e5ab-4e41-4989-871f-6481c16a79d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, LSTM, Input, Activation, concatenate\n",
    "from keras import optimizers\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# define two sets of inputs\n",
    "lstm_input = Input(shape=(history_points, 5), name='lstm_input')\n",
    "dense_input = Input(shape=(technical_indicators.shape[1],), name='tech_input')\n",
    " \n",
    "# the first branch operates on the first input\n",
    "x = LSTM(50, name='lstm_0')(lstm_input)\n",
    "x = Dropout(0.2, name='lstm_dropout_0')(x)\n",
    "lstm_branch = Model(inputs=lstm_input, outputs=x)\n",
    " \n",
    "# the second branch opreates on the second input\n",
    "y = Dense(20, name='tech_dense_0')(dense_input)\n",
    "y = Activation(\"relu\", name='tech_relu_0')(y)\n",
    "y = Dropout(0.2, name='tech_dropout_0')(y)\n",
    "technical_indicators_branch = Model(inputs=dense_input, outputs=y)\n",
    " \n",
    "# combine the output of the two branches\n",
    "combined = concatenate([lstm_branch.output, technical_indicators_branch.output], name='concatenate')\n",
    " \n",
    "z = Dense(64, activation=\"sigmoid\", name='dense_pooling')(combined)\n",
    "z = Dense(1, activation=\"linear\", name='dense_out')(z)\n",
    " \n",
    "# our model will accept the inputs of the two branches and then output a single value\n",
    "model = Model(inputs=[lstm_branch.input, technical_indicators_branch.input], outputs=z)\n",
    "\n",
    "adam = optimizers.Adam(lr=0.0005)\n",
    "\n",
    "model.compile(optimizer=adam,\n",
    "              loss='mse')\n",
    "\n",
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e803e634-2f5f-45ac-add6-03cadc6f612d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=[ohlcv_train, tech_ind_train], y=y_train, batch_size=32, epochs=50, shuffle=True, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc7bacd-f227-42b6-b9a2-25b90bc645a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "<keras.callbacks.History at 0x7f40860de978>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525afc29-83bc-4ce2-8803-000856322056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Las predicciones estásn normalizadas\n",
    "y_test_predicted = model.predict([ohlcv_test, tech_ind_test])\n",
    "\n",
    "y_test_predicted = y_scaler.inverse_transform(y_test_predicted)\n",
    "\n",
    "real_mse = np.mean(np.square(unscaled_y_test - y_test_predicted))\n",
    "scaled_mse = real_mse / (np.max(unscaled_y_test) - np.min(unscaled_y_test)) * 100\n",
    "print(scaled_mse)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3709a986-a2e2-4261-8477-fa7885a02e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.gcf().set_size_inches(22, 15, forward=True)\n",
    "\n",
    "start = 0\n",
    "end = -1\n",
    "\n",
    "real = plt.plot(unscaled_y_test[start:end], label='real')\n",
    "pred = plt.plot(y_test_predicted[start:end], label='predicted')\n",
    "\n",
    "plt.legend(['Real', 'Predicted'])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc1b85e-5384-4923-b8ca-dae800d43fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "buys = []\n",
    "sells = []\n",
    "thresh = 0.01#0.2\n",
    "\n",
    "x = 0\n",
    "for ohlcv, ind in zip(ohlcv_test, tech_ind_test):\n",
    "    normalised_price_today = ohlcv[-1][0]\n",
    "    normalised_price_today = np.array([[normalised_price_today]])\n",
    "    price_today = y_scaler.inverse_transform(normalised_price_today)\n",
    "    predicted = np.squeeze(y_scaler.inverse_transform( model.predict([[ohlcv], [ind]])))\n",
    "    delta = predicted - price_today\n",
    "    # print(delta)\n",
    "    if delta > thresh:\n",
    "        buys.append((x, price_today[0][0]))\n",
    "    elif delta < -thresh:\n",
    "        sells.append((x, price_today[0][0]))\n",
    "    x += 1\n",
    "print(len(buys))\n",
    "print(len(sells))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb915bb-c7f6-4c0a-8d55-b317590c3181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.gcf().set_size_inches(22, 15, forward=True)\n",
    "\n",
    "start = 0\n",
    "end = -1\n",
    "\n",
    "real = plt.plot(unscaled_y_test[start:end], label='real')\n",
    "pred = plt.plot(y_test_predicted[start:end], label='predicted')\n",
    "\n",
    "plt.scatter(list(list(zip(*buys))[0]), list(list(zip(*buys))[1]), c='#00ff00')  # verde\n",
    "plt.scatter(list(list(zip(*sells))[0]), list(list(zip(*sells))[1]), c='#ff0000') # rojo\n",
    "\n",
    "# real = plt.plot(unscaled_y[start:end], label='real')\n",
    "# pred = plt.plot(y_predicted[start:end], label='predicted')\n",
    "\n",
    "plt.legend(['Real', 'Predicted'])\n",
    "\n",
    "\n",
    "plt.savefig('predicciones.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a13cf2a-e74a-480c-bc81-da85f69c44be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_earnings(buys, sells):\n",
    "    purchase_amt = 10\n",
    "    stock = 0\n",
    "    balance = 0\n",
    "    while len(buys) > 0 and len(sells) > 0:\n",
    "        if buys[0][0] < sells[0][0]:\n",
    "            # time to buy $10 worth of stock\n",
    "            balance -= purchase_amt\n",
    "            stock += purchase_amt / buys[0][1]\n",
    "            buys.pop(0)\n",
    "        else:\n",
    "            # time to sell all of our stock\n",
    "            balance += stock * sells[0][1]\n",
    "            stock = 0\n",
    "            sells.pop(0)\n",
    "    print(balance)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e64f10-d3d0-4929-b7ae-b4c4038df606",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_earnings(buys, sells)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
